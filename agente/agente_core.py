"""
ü§ñ Agente de An√°lise Explorat√≥ria de Dados (EDA)

Este m√≥dulo implementa um agente inteligente que interpreta perguntas em linguagem
natural sobre dados e executa an√°lises estat√≠sticas, visualiza√ß√µes e an√°lises avan√ßadas.

Principais funcionalidades:
- Interpreta√ß√£o de linguagem natural para an√°lise de dados
- Execu√ß√£o automatizada de an√°lises estat√≠sticas
- Gera√ß√£o de visualiza√ß√µes interativas
 - Aplica√ß√£o de algoritmos para an√°lises avan√ßadas (clustering, PCA, regress√£o)
- Mem√≥ria conversacional para contexto

Autor: Sistema de An√°lise Explorat√≥ria
Vers√£o: 2.0
"""

import os
import re
import tempfile
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional

import google.generativeai as genai
import pandas as pd
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.memory import ConversationBufferMemory
from langchain.schema import BaseMessage
from langchain.tools import Tool, tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import SecretStr

# M√≥dulos locais
from agente.ferramentas import criar_ferramentas_agente
from agente.memoria import MemoriaConversa
from config_manager import get_config, get_api_key

# Flag de disponibilidade
LANGCHAIN_AVAILABLE = True


class AgenteEDA:
    """
    Agente principal para An√°lise Explorat√≥ria de Dados.
    Utiliza LangChain para interpretar perguntas e executar an√°lises.
    """
    
    def __init__(self, dataframe: pd.DataFrame, api_key: Optional[str] = None, llm_model: Optional[str] = None):
        """
        Inicializa o agente EDA.
        
        Args:
            dataframe: DataFrame para an√°lise
            api_key: Chave da API Google Gemini (opcional, pode vir do ambiente)
            llm_model: Nome do modelo LLM a ser utilizado (opcional)
        """
        self.df = dataframe
        self.memoria = MemoriaConversa()
        self.llm_model = llm_model
        
        # Configurar API key do Google Gemini
        # Sempre atualiza a vari√°vel de ambiente se uma nova API key √© fornecida
        if api_key:
            os.environ['GOOGLE_API_KEY'] = api_key
        
        # Verificar se temos uma API key dispon√≠vel (prioridade: par√¢metro -> secrets -> .env)
        current_api_key = api_key or get_api_key()
        if not current_api_key:
            raise ValueError("ERRO DE AUTENTICA√á√ÉO: Chave API do Google Gemini n√£o encontrada.\n\n"
                           "CONFIGURA√á√ÉO NECESS√ÅRIA:\n"
                           "1. Obtenha uma chave API gratuita: https://makersuite.google.com/app/apikey\n"
                           "2. Insira a chave no campo 'Chave da API do Motor IA' (barra lateral)\n\n"
                           "NOTA: A API do Google Gemini oferece n√≠vel gratuito para uso b√°sico.\n\n"
                           "SUPORTE T√âCNICO: Contate o desenvolvedor Igor T√∂ebe para assist√™ncia.")
        
        if not LANGCHAIN_AVAILABLE:
            raise ImportError("ERRO DE DEPEND√äNCIA: Framework LangChain n√£o dispon√≠vel.\n\n"
                            "INSTALA√á√ÉO NECESS√ÅRIA:\n"
                            "pip install langchain langchain-google-genai\n\n"
                            "SUPORTE T√âCNICO: Contate o desenvolvedor Igor T√∂ebe para assist√™ncia.")
        
        self.llm_disponivel = True
        print("STATUS DO SISTEMA: Inicializa√ß√£o do Agente IA em progresso...")
        
        # Inicializar componentes do agente
        self._inicializar_agente_langchain()
    
    def _inicializar_agente_langchain(self):
        """Inicializa o agente usando LangChain com Google Gemini ou outro modelo."""
        try:
            api_key_str = get_api_key()
            # Prioriza modelo passado pelo construtor
            model_name = self.llm_model or get_config('GEMINI_MODEL', 'gemini-2.5-flash')
            temperature = float(get_config('GEMINI_TEMPERATURE', '0.1'))
            self.llm = ChatGoogleGenerativeAI(
                model=model_name,
                temperature=temperature,
                api_key=SecretStr(api_key_str) if api_key_str else None,
                convert_system_message_to_human=True
            )
            
            # Criar ferramentas
            self.ferramentas = criar_ferramentas_agente(self.df)
            
            # Criar prompt do agente
            prompt = ChatPromptTemplate.from_messages([
                ("system", self._get_prompt_sistema()),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad")
            ])
            
            # Criar agente (usando create_tool_calling_agent que funciona com Gemini)
            agent = create_tool_calling_agent(
                llm=self.llm,
                tools=self.ferramentas,
                prompt=prompt
            )
            
            # Criar executor
            self.agent_executor = AgentExecutor(
                agent=agent,
                tools=self.ferramentas,
                memory=self.memoria.get_langchain_memory(),
                verbose=False,  # Desativado para evitar exposi√ß√£o de c√≥digo interno
                max_iterations=20,  # Aumentado para permitir an√°lises mais complexas
                early_stopping_method="generate",  # Permite parada antecipada quando o objetivo √© alcan√ßado
                return_intermediate_steps=True,  # IMPORTANTE: precisamos dos steps para capturar gr√°ficos
                handle_parsing_errors=True       # Torna o agente mais resiliente a pequenas falhas de parsing
            )
            
            print("STATUS DO SISTEMA: Agente IA inicializado com sucesso e pronto para an√°lise.")
            
        except Exception as e:
            print(f"ERRO DE INICIALIZA√á√ÉO: {e}")
            # Verificar se √© um erro de API key inv√°lida
            erro_str = str(e).lower()
            if "api_key_invalid" in erro_str or "api key not valid" in erro_str:
                raise ValueError("ERRO DE AUTENTICA√á√ÉO: Chave API do Google Gemini inv√°lida.\n\n"
                               "LISTA DE VERIFICA√á√ÉO:\n"
                               "1. Certifique-se de que a chave foi copiada corretamente (sem espa√ßos extras)\n"
                               "2. Verifique se a chave est√° ativa: https://makersuite.google.com/app/apikey\n"
                               "3. Confirme se h√° cota/cr√©ditos da API dispon√≠veis\n\n"
                               "RECOMENDA√á√ÉO: Teste a chave em https://makersuite.google.com primeiro.\n\n"
                               "SUPORTE T√âCNICO: Contate o desenvolvedor Igor T√∂ebe para assist√™ncia.")
            else:
                raise RuntimeError(f"ERRO DO SISTEMA: Falha ao inicializar o motor IA: {e}\n\n"
                                 "Por favor, verifique a conex√£o com a internet e tente novamente.\n\n"
                                 "SUPORTE T√âCNICO: Contate o desenvolvedor Igor T√∂ebe se o problema persistir.")
    
    def _get_prompt_sistema(self) -> str:
        """Retorna um prompt de sistema conciso e focado em ferramentas."""
        colunas_numericas = self.df.select_dtypes(include=['number']).columns.tolist()
        colunas_categoricas = self.df.select_dtypes(include=['object', 'category']).columns.tolist()

        todas_colunas = ", ".join(list(self.df.columns))
        cols_num = ", ".join(colunas_numericas) if colunas_numericas else "Nenhuma"
        cols_cat = ", ".join(colunas_categoricas) if colunas_categoricas else "Nenhuma"

        return f"""
Voc√™ √© um analista de dados IA avan√ßado com capacidades profissionais de an√°lise estat√≠stica e visualiza√ß√£o.

PERFIL DO DATASET:
- Registros: {len(self.df):,} | Caracter√≠sticas: {len(self.df.columns)}
- Caracter√≠sticas Dispon√≠veis: {todas_colunas}
- Caracter√≠sticas Num√©ricas: {cols_num}
- Caracter√≠sticas Categ√≥ricas: {cols_cat}

FRAMEWORK OPERACIONAL:
1) Execute ferramentas diretamente - nunca sugira c√≥digo ou procedimentos manuais
2) APENAS crie visualiza√ß√µes quando EXPLICITAMENTE solicitado pelo usu√°rio
3) Use exatamente os nomes das caracter√≠sticas do perfil do dataset acima
4) Forne√ßa interpreta√ß√µes estat√≠sticas profissionais e concisas
5) Foque em insights acion√°veis e observa√ß√µes de qualidade dos dados

SUAS CAPACIDADES (use linguagem natural, n√£o nomes de fun√ß√µes):
- An√°lise de distribui√ß√£o: "Criando histograma para mostrar a distribui√ß√£o dos dados"
- Detec√ß√£o de outliers: "Detectando valores at√≠picos e criando boxplots para visualiz√°-los"
- An√°lise de correla√ß√£o: "Calculando correla√ß√µes e criando gr√°ficos de dispers√£o ou mapas de calor"
- An√°lise de frequ√™ncia: "Analisando a frequ√™ncia de valores √∫nicos"
- Testes estat√≠sticos: "Realizando testes de normalidade, testes t, correla√ß√£o, qui-quadrado e ANOVA"
- An√°lise comparativa: "Comparando grupos e criando visualiza√ß√µes segmentadas"
- An√°lises avan√ßadas: "Realizando clustering, PCA, regress√£o e classifica√ß√£o com resultados detalhados"
- Vis√£o geral: "Descrevendo o dataset e listando suas caracter√≠sticas"

IMPORTANTE PARA AN√ÅLISES AVAN√áADAS:
- SEMPRE analise e apresente TODOS os resultados num√©ricos das an√°lises avan√ßadas
- Para clustering: mostre n√∫mero de clusters, silhouette score, distribui√ß√£o e caracter√≠sticas
- Para PCA: mostre vari√¢ncia explicada, componentes principais e m√©tricas de redu√ß√£o
- Para regress√£o/classifica√ß√£o: mostre m√©tricas de performance, coeficientes e interpreta√ß√µes
- NUNCA omita dados quantitativos dos resultados

COMUNICA√á√ÉO NATURAL:
- EXECUTE IMEDIATAMENTE as a√ß√µes solicitadas - N√ÉO prometa que "vai fazer"
- Use frases como: "Calculando...", "Analisando...", "Criando gr√°fico de..."
- NUNCA mencione nomes de fun√ß√µes como "gerar_histograma_coluna()" ou "calcular_matriz_correlacao()"
- Fale como um analista profissional, n√£o como um programador
- FA√áA AGORA, n√£o descreva o que vai fazer

LINGUAGEM PROIBIDA (nunca use):
‚ùå "Vou usar a fun√ß√£o gerar_grafico_dispersao()"
‚ùå "Posso executar calcular_matriz_correlacao()"
‚ùå "Vamos chamar detectar_outliers_coluna()"

LINGUAGEM CORRETA (sempre use):
‚úÖ "Criando gr√°fico de dispers√£o..."
‚úÖ "Calculando matriz de correla√ß√£o..."
‚úÖ "Detectando valores at√≠picos..."

CAPACIDADES ESTAT√çSTICAS:
- Testes de normalidade (Shapiro-Wilk, D'Agostino-Pearson, Jarque-Bera, Anderson-Darling)
- Testes de hip√≥teses (testes t, testes de correla√ß√£o, qui-quadrado, ANOVA)
- Testes n√£o-param√©tricos (Mann-Whitney U)
- Estat√≠sticas descritivas e an√°lise de distribui√ß√£o

PROTOCOLO DE RESPOSTA:
- Converse com o usu√°rio de forma profissional e amig√°vel
- Ap√≥s a primeira intera√ß√£o descreva suas capacidades e realize uma pr√© an√°lise r√°pida para entender o dataset e contexto
-- SEMPRE que solicitado, execute as ferramentas solicitadas - voc√™ tem TODAS as capacidades necess√°rias
-- Para solicita√ß√µes EXPL√çCITAS de gr√°ficos ("gere histograma", "crie gr√°fico", etc.), USE IMEDIATAMENTE a ferramenta apropriada
-- NUNCA gere gr√°ficos automaticamente sem solicita√ß√£o expl√≠cita do usu√°rio
-- Para an√°lises de clustering, SEMPRE apresente TODOS os dados e resultados obtidos
- Comunique-se como um ANALISTA PROFISSIONAL, n√£o como um programador
- JAMAIS diga "Vou fazer", "Vou analisar", "Vou realizar" - EXECUTE IMEDIATAMENTE quando solicitado!
- Use linguagem de A√á√ÉO PRESENTE: "Calculando matriz de correla√ß√£o..." ao inv√©s de "Vou calcular..."
- NUNCA inclua c√≥digo Python, print(), ou comandos t√©cnicos na resposta
- NUNCA mostre chamadas de fun√ß√£o como "default_api.realizar_clustering()"
- APRESENTE COMPLETAMENTE todos os resultados num√©ricos, m√©tricas e interpreta√ß√µes de ML
- Para clustering: sempre mostre silhouette score, n√∫mero de clusters, distribui√ß√£o e caracter√≠sticas dos grupos
- Para PCA: sempre mostre vari√¢ncia explicada por componente, redu√ß√£o de dimensionalidade e loadings
- Para modelos supervisionados: sempre mostre m√©tricas de performance e coeficientes/import√¢ncia
- Seja um bom comunicador, sempre escute atentamente o usu√°rio e responda de forma clara e amig√°vel
- Forne√ßa interpreta√ß√£o estat√≠stica profissional (3-6 frases)
- Destaque achados significativos, padr√µes ou quest√µes de qualidade dos dados
- Recomende an√°lises de acompanhamento quando apropriado
- Nunca referencie caminhos de arquivos, nomes de fun√ß√µes ou detalhes de implementa√ß√£o t√©cnica

-- A√á√ïES IMEDIATAS OBRIGAT√ìRIAS:
-- Quando solicitado clustering: EXECUTE imediatamente, apresente os resultados completos
-- Quando solicitada an√°lise: FA√áA a an√°lise agora, n√£o prometa que far√°
-- Quando solicitado gr√°fico: CRIE o gr√°fico imediatamente, n√£o explique que pode criar
-- PROIBIDO: Frases como "Aguarde um momento...", "Vou realizar...", "Ap√≥s a an√°lise..."
-- OBRIGAT√ìRIO: Execute, analise e apresente resultados de uma s√≥ vez
"""
    
    def processar_pergunta(self, pergunta: str) -> Dict[str, Any]:
        """
        Processa uma pergunta do usu√°rio e retorna a resposta.
        
        Args:
            pergunta: Pergunta em linguagem natural
            
        Returns:
            Dicion√°rio com resposta, gr√°ficos gerados e metadados
        """
        try:
            # Adicionar √† mem√≥ria
            self.memoria.adicionar_interacao(pergunta, None)
            
            # Como agora sempre temos LLM dispon√≠vel (ou o agente nem inicializa)
            return self._processar_com_langchain(pergunta)
                
        except Exception as e:
            erro_msg = f"Erro ao processar pergunta: {str(e)}"
            print(f"‚ùå {erro_msg}")
            print(traceback.format_exc())
            
            # Verificar se √© um erro que pode indicar limita√ß√£o de funcionalidade
            erro_str = str(e).lower()
            if any(termo in erro_str for termo in ['not supported', 'n√£o suportado', 'cannot', 'n√£o consegue', 'unable']):
                texto_erro = (
                    "**Funcionalidade n√£o dispon√≠vel no momento.**\n\n"
                    "Esta solicita√ß√£o n√£o pode ser realizada com as ferramentas atuais.\n\n"
                    "**Para novas funcionalidades:** Contate o desenvolvedor Igor T√∂ebe para adicionar esta funcionalidade ao sistema."
                )
            else:
                texto_erro = (
                    "**Erro t√©cnico tempor√°rio.**\n\n"
                    "Ocorreu um erro ao processar sua pergunta. Tente novamente ou reformule sua solicita√ß√£o.\n\n"
                    "**Se o problema persistir:** Contate o desenvolvedor Igor T√∂ebe para suporte t√©cnico."
                )
            
            return {
                'texto': texto_erro,
                'erro': erro_msg
            }
    
    def _processar_com_langchain(self, pergunta: str) -> Dict[str, Any]:
        """Processa pergunta usando LangChain com Google Gemini."""
        try:
            # Executar agente
            resultado = self.agent_executor.invoke({
                "input": pergunta,
                "chat_history": self.memoria.get_historico_formatado()
            })
            
            resposta_texto = resultado.get('output', 'Sem resposta')
            
            # Filtrar c√≥digo t√©cnico da resposta
            resposta_texto = self._limpar_resposta_tecnica(resposta_texto)
            
            # Procurar por caminhos de gr√°ficos na resposta e nos intermediate_steps
            graficos = []
            
            # Buscar na resposta final por caminhos de gr√°fico
            # Regex atualizada para incluir par√™nteses e espa√ßos em nomes de arquivo
            graficos_resposta = re.findall(r'temp_graficos[/\\][\w\-_.()\s]+\.png', resposta_texto)
            graficos.extend(graficos_resposta)
            
            # Procurar nos intermediate_steps (sa√≠das das ferramentas)
            intermediate_steps = resultado.get('intermediate_steps', [])
            dados_ml_completos = []
            
            for step in intermediate_steps:
                if len(step) > 1:
                    step_output = step[1]
                    
                    # 1) Quando a ferramenta retorna string: extrair caminhos e dados
                    if isinstance(step_output, str):
                        # Regex atualizada para incluir par√™nteses e espa√ßos em nomes de arquivo
                        step_graficos = re.findall(r'temp_graficos[/\\][\w\-_.()\s]+\.png', step_output)
                        graficos.extend(step_graficos)
                        if any(palavra in step_output.lower() for palavra in ['clustering', 'pca', 'regress√£o', 'classifica√ß√£o', 'silhouette', 'vari√¢ncia']):
                            dados_ml_completos.append(step_output)
                    
                    # 2) Quando a ferramenta retorna dict: buscar chaves comuns com paths de imagem
                    elif isinstance(step_output, dict):
                        # Tentar coletar caminho(s) de gr√°fico por chaves comuns
                        for k in ['path', 'arquivo', 'grafico', 'file', 'figure_path', 'image', 'plot']:
                            if k in step_output:
                                val = step_output[k]
                                if isinstance(val, str) and val.lower().endswith('.png'):
                                    graficos.append(val)
                                elif isinstance(val, list):
                                    for v in val:
                                        if isinstance(v, str) and v.lower().endswith('.png'):
                                            graficos.append(v)
                        # Agregar poss√≠veis textos/relat√≥rios de ML
                        for k in ['relatorio', 'resultado', 'texto', 'report', 'details']:
                            if k in step_output and isinstance(step_output[k], str):
                                if any(p in step_output[k].lower() for p in ['clustering', 'pca', 'regress√£o', 'classifica√ß√£o', 'silhouette', 'vari√¢ncia']):
                                    dados_ml_completos.append(step_output[k])
            
            # Se temos dados de ML nos intermediate_steps, garantir que est√£o na resposta final
            if dados_ml_completos and not any(palavra in resposta_texto.lower() for palavra in ['clustering', 'pca', 'silhouette', 'vari√¢ncia']):
                resposta_texto += '\n\n' + '\n\n'.join(dados_ml_completos)
            
            # REMOVIDO: Busca autom√°tica de gr√°ficos que causava exibi√ß√£o n√£o solicitada
            # A busca por gr√°ficos agora ocorre APENAS se explicitamente mencionados na resposta
            
            # Remover duplicatas mantendo ordem e verificar se arquivos existem
            graficos_validos = []
            for grafico in graficos:
                caminho_norm = str(Path(grafico))
                if caminho_norm not in graficos_validos and os.path.exists(caminho_norm):
                    graficos_validos.append(caminho_norm)
            
            graficos = graficos_validos
            
            # Se temos m√∫ltiplos gr√°ficos, priorizar o mais recente
            if len(graficos) > 1:
                graficos.sort(key=lambda p: Path(p).stat().st_mtime, reverse=True)
            
            # Atualizar mem√≥ria
            self.memoria.adicionar_interacao(pergunta, resposta_texto)
            
            response = {
                'texto': resposta_texto,
                'graficos': graficos,
                'grafico': graficos[0] if graficos else None,
                'metadados': {
                    'modelo': 'google-gemini',
                    'ferramentas_usadas': resultado.get('intermediate_steps', [])
                }
            }
            
            return response
            
        except Exception as e:
            erro_str = str(e)
            
            # Verificar tipo de erro para dar resposta adequada
            if "API key not valid" in erro_str or "API_KEY_INVALID" in erro_str:
                texto_erro = (
                    "**Erro de autentica√ß√£o com Google Gemini**\n\n"
                    "Sua chave API n√£o √© v√°lida ou expirou.\n\n" 
                    "**Solu√ß√£o:** Verifique sua chave API em https://makersuite.google.com/app/apikey\n\n"
                    "**Precisa de ajuda?** Contate o desenvolvedor Igor T√∂ebe para suporte t√©cnico."
                )
            elif "quota" in erro_str.lower() or "limit" in erro_str.lower():
                texto_erro = (
                    "**Limite de uso atingido**\n\n"
                    "Voc√™ atingiu o limite da API do Google Gemini.\n\n"
                    "**Solu√ß√£o:** Aguarde um tempo ou verifique sua cota em https://console.cloud.google.com\n\n"
                    "**Precisa de ajuda?** Contate o desenvolvedor Igor T√∂ebe para orienta√ß√£o."
                )
            else:
                texto_erro = (
                    "**Erro t√©cnico no processamento**\n\n"
                    "Ocorreu um problema t√©cnico ao processar sua solicita√ß√£o.\n\n"
                    "**Tente:** Reformular sua pergunta ou tentar novamente\n\n"
                    "**Se persistir:** Contate o desenvolvedor Igor T√∂ebe para suporte t√©cnico."
                )
            
            return {
                'texto': texto_erro,
                'erro': erro_str
            }
    

    
    def get_sugestoes_contextuais(self) -> List[str]:
        """Retorna sugest√µes baseadas no contexto atual dos dados e capacidades reais do agente."""
        sugestoes = []
        
        # Sugest√µes baseadas nas colunas dispon√≠veis
        colunas_numericas = self.df.select_dtypes(include=['number']).columns.tolist()
        colunas_categoricas = self.df.select_dtypes(include=['object']).columns.tolist()
        
        # Sugest√£o priorit√°ria: an√°lise de tipos de colunas
        sugestoes.append("Sugira quais colunas podem ser categ√≥ricas")
        
        # Sugest√µes de visualiza√ß√£o espec√≠ficas por coluna
        if colunas_numericas:
            primeira_col = colunas_numericas[0]
            sugestoes.extend([
                f"Gere um histograma da coluna '{primeira_col}'",
                f"Detecte outliers em '{primeira_col}'",
                f"Analise estat√≠sticas de '{primeira_col}'"
            ])
            
            if len(colunas_numericas) > 1:
                segunda_col = colunas_numericas[1]
                sugestoes.append(f"Crie gr√°fico de dispers√£o entre '{primeira_col}' e '{segunda_col}'")
        
        if colunas_categoricas:
            primeira_cat = colunas_categoricas[0]
            sugestoes.append(f"Analise frequ√™ncias em '{primeira_cat}'")
            
            # Sugest√£o de an√°lise por grupos se tiver num√©rica e categ√≥rica
            if colunas_numericas and colunas_categoricas:
                sugestoes.append(f"Compare '{colunas_numericas[0]}' por grupos de '{primeira_cat}'")
        
        # An√°lises avan√ßadas dispon√≠veis
        sugestoes.extend([
            "Calcule matriz de correla√ß√£o com heatmap",
            "Descreva o dataset completo"
        ])
        
        return sugestoes[:8]  # Expandir para 8 sugest√µes
    
    def _limpar_resposta_tecnica(self, resposta: str) -> str:
        """Remove c√≥digo t√©cnico e debugging da resposta do agente, preservando resultados de ML."""
        # Padr√µes de c√≥digo t√©cnico a serem removidos (mais espec√≠ficos)
        padroes_tecnicos = [
            r'^print\(.*?\)$',  # Comandos print isolados
            r'^default_api\.\w+\([^)]*\)$',  # Chamadas de API interna isoladas
            r'^tool\.\w+\([^)]*\)$',  # Chamadas de ferramentas t√©cnicas isoladas
            r'^function\([^)]*\)$',  # Chamadas gen√©ricas de fun√ß√£o isoladas
            r'^>>>.*?$',  # Prompts de Python
            r'^>>>\s+.*?$',  # Prompts de Python com espa√ßos
            r'^Traceback \(most recent call last\):.*?$',  # Stack traces espec√≠ficos
            r'^\s*File ".*?", line \d+.*?$',  # Linhas de stack trace
            r'^\s*\w+Error:.*?$',  # Erros espec√≠ficos do Python isolados
        ]
        
        linhas = resposta.split('\n')
        linhas_limpas = []
        
        for linha in linhas:
            linha_limpa = linha.strip()
            
            # Verificar se a linha cont√©m c√≥digo t√©cnico
            contem_codigo_tecnico = False
            for padrao in padroes_tecnicos:
                if re.match(padrao, linha_limpa, re.IGNORECASE):
                    contem_codigo_tecnico = True
                    break
            
            # Preservar linhas importantes dos resultados de ML
            if not contem_codigo_tecnico:
                linhas_limpas.append(linha)
        
        return '\n'.join(linhas_limpas)
    
    def get_info_dataset(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes b√°sicas sobre o dataset."""
        return {
            'linhas': len(self.df),
            'colunas': len(self.df.columns),
            'colunas_numericas': self.df.select_dtypes(include=['number']).columns.tolist(),
            'colunas_categoricas': self.df.select_dtypes(include=['object', 'category']).columns.tolist(),
            'memoria_mb': self.df.memory_usage(deep=True).sum() / (1024**2),
            'valores_nulos': self.df.isnull().sum().sum()
        }